{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaU0XILcpQDKbNM5W/dJUr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaehnK/GoogleCSE_Webometrics/blob/main/googleCSE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 분석에 필요한 패키지 로드"
      ],
      "metadata": {
        "id": "GwouMJ_RB6ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def install_and_import(package):\n",
        "    import importlib\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "    except ImportError:\n",
        "        import subprocess\n",
        "        print(f\"{package} Not Installed... Start Install\")\n",
        "        subprocess.check_call([\"pip\", \"install\", package])\n",
        "        print(f\"{package} Sucessfully Installed.\")\n",
        "    finally:\n",
        "        globals()[package] = importlib.import_module(package)"
      ],
      "metadata": {
        "id": "QqoHzWRLaE91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVSY-HqiUZlN"
      },
      "outputs": [],
      "source": [
        "install_and_import(\"requests\")\n",
        "install_and_import(\"pandas\")\n",
        "install_and_import(\"tldextract\")\n",
        "install_and_import(\"matplotlib\")\n",
        "install_and_import(\"seaborn\")\n",
        "\n",
        "from collections import Counter\n",
        "import csv\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "from typing import List\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import pandas as pd\n",
        "import tldextract\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "FILE_PATH = \"/content/drive/MyDrive\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GoogleCSE 객체 선언"
      ],
      "metadata": {
        "id": "F7QfDd-SB-VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class googlecse():\n",
        "\n",
        "  def __init__(self):\n",
        "    self._API_KEY = None\n",
        "    self._SEARCH_ENGINE_ID = None\n",
        "    self._query = None\n",
        "    self._response_list = None\n",
        "    self._tlds = None\n",
        "    self._stlds = None\n",
        "    self._domains = None\n",
        "    self._count_tlds = None\n",
        "    self._count_stlds = None\n",
        "    self._count_domains = None\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"API_KEY: {self._API_KEY}\\nSEARCH_ENGINE_ID: {self._SEARCH_ENGINE_ID}\\nquery: {self._query}\"\n",
        "\n",
        "  @property\n",
        "  def API_KEY(self):\n",
        "      return self._API_KEY\n",
        "\n",
        "  @API_KEY.setter\n",
        "  def API_KEY(self, value: str):\n",
        "      if not value or not isinstance(value, str):\n",
        "          raise ValueError(\"API_KEY must be a non-empty string\")\n",
        "      if len(value) < 10:\n",
        "          raise ValueError(\"API_KEY is too short. It should be at least 10 characters long\")\n",
        "      self._API_KEY = value\n",
        "\n",
        "  @property\n",
        "  def SEARCH_ENGINE_ID(self):\n",
        "      return self._SEARCH_ENGINE_ID\n",
        "\n",
        "  @SEARCH_ENGINE_ID.setter\n",
        "  def SEARCH_ENGINE_ID(self, value: str):\n",
        "      if not value or not isinstance(value, str):\n",
        "          raise ValueError(\"SEARCH_ENGINE_ID must be a non-empty string\")\n",
        "      self._SEARCH_ENGINE_ID = value\n",
        "\n",
        "  @property\n",
        "  def query(self):\n",
        "      return self._query\n",
        "\n",
        "  @query.setter\n",
        "  def query(self, value: str):\n",
        "      if not value or not isinstance(value, str):\n",
        "          raise ValueError(\"Query must be a non-empty string\")\n",
        "      if len(value) < 2:\n",
        "          raise ValueError(\"Query is too short. It should be at least 2 characters long\")\n",
        "      self._query = value\n",
        "\n",
        "\n",
        "  def setup_search_parameters(self):\n",
        "      \"\"\"\n",
        "      사용자 입력을 통해 Google Custom Search API에 필요한 주요 매개변수를 설정합니다.\n",
        "\n",
        "      이 메서드는 API_KEY, SEARCH_ENGINE_ID, 검색 쿼리를 사용자로부터 입력받아 설정합니다.\n",
        "      각 입력값은 setter 메서드를 통해 유효성 검사를 거친 후 설정됩니다.\n",
        "\n",
        "      Returns:\n",
        "          None: 오류 발생 시 None을 반환, 성공 시 반환값 없음\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      try:\n",
        "          self.API_KEY = input(\"API_KEY: \")\n",
        "          self.SEARCH_ENGINE_ID = input(\"SEARCH_ENGINE_ID: \")\n",
        "          self.query = input(\"Query: \")\n",
        "      except ValueError as e:\n",
        "          print(f\"Error: {e}\")\n",
        "          return\n",
        "\n",
        "  def get_default_keys(self):\n",
        "      self._API_KEY = \"AIzaSyDhZmbnXs-i1Bi97X9S50_UwW09ZjyiZXU\"\n",
        "      self._SEARCH_ENGINE_ID = \"3178b564dfd2e4dac\"\n",
        "\n",
        "  def response_oneline(self):\n",
        "    \"\"\"\n",
        "    Google Custom Search API를 사용하여 단일 페이지의 검색 결과를 가져옵니다.\n",
        "\n",
        "    API를 호출하여 첫 번째 페이지의 검색 결과를 가져온 후, 두 번째 결과 항목의\n",
        "    모든 키-값 쌍을 출력합니다. 주로 API 응답 구조를 확인하는 용도로 사용됩니다.\n",
        "\n",
        "    필수 전제조건: API_KEY, SEARCH_ENGINE_ID, query가 설정되어 있어야 합니다.\n",
        "\n",
        "    Returns:\n",
        "        dict: API 응답 전체를 JSON 형태로 반환\n",
        "        None: 검색 결과가 없거나 오류 발생 시 None 반환\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if (self._API_KEY is None or self._SEARCH_ENGINE_ID is None or self._query is None):\n",
        "      raise ValueError(\"API_KEY, SEARCH_ENGINE_ID, and query must be set\")\n",
        "\n",
        "    idx = 1\n",
        "    url = f\"https://www.googleapis.com/customsearch/v1?key={self._API_KEY}&cx={self._SEARCH_ENGINE_ID}&q={self._query}&start={idx}\"\n",
        "    response = requests.get(url).json()\n",
        "    if ('items' not in response.keys()):\n",
        "        print(\"Error: Items not in response\")\n",
        "        return (None)\n",
        "\n",
        "    for key in response['items'][1].keys():\n",
        "      print(f\"{key:<15}: {response['items'][1][key]}\")\n",
        "      print(\"\\u2501\" * 80)\n",
        "\n",
        "    return response\n",
        "\n",
        "  def response(self)-> List[str]:\n",
        "    \"\"\"\n",
        "    Google Custom Search API를 사용하여 모든 검색 결과의 URL을 수집합니다.\n",
        "\n",
        "    페이지네이션을 활용하여 API의 모든 결과 페이지를 순회하면서\n",
        "    각 검색 결과의 링크(URL)를 수집하여 리스트로 저장합니다.\n",
        "    API 호출 사이에는 0.5초의 지연을 두어 API 사용량 제한을 고려합니다.\n",
        "\n",
        "    필수 전제조건: API_KEY, SEARCH_ENGINE_ID, query가 설정되어 있어야 합니다.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: 수집된 모든 URL의 리스트\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if (self._API_KEY is None or self._SEARCH_ENGINE_ID is None or self._query is None):\n",
        "      raise ValueError(\"API_KEY, SEARCH_ENGINE_ID, and query must be set\")\n",
        "\n",
        "    self._response_list = []\n",
        "    idx = 1\n",
        "\n",
        "    while (True):\n",
        "      url = f\"https://www.googleapis.com/customsearch/v1?key={self._API_KEY}&cx={self._SEARCH_ENGINE_ID}&q={self._query}&start={idx}\"\n",
        "      response = requests.get(url).json()\n",
        "\n",
        "      if 'items' not in response.keys():\n",
        "          print(\"Items not in response\")\n",
        "          break;\n",
        "\n",
        "      len_items = len(response['items'])\n",
        "\n",
        "      if (len_items == 0):\n",
        "          print(\"No more results\")\n",
        "          break;\n",
        "\n",
        "      for i in range(len_items):\n",
        "          self._response_list.append(response['items'][i]['link'])\n",
        "\n",
        "      print(f\"{len(self._response_list)} items collected\")\n",
        "\n",
        "      idx += len_items\n",
        "      time.sleep(.5)\n",
        "\n",
        "    print(f\"GoogleCSE: {len(self._response_list)} links are collected.\")\n",
        "    return self._response_list\n",
        "\n",
        "  def extract_tld(self):\n",
        "    \"\"\"\n",
        "    수집된 URL 목록에서 최상위 도메인(TLD)만 추출합니다.\n",
        "\n",
        "    각 URL에서 tldextract 라이브러리를 사용하여 최상위 도메인(예: com, org, net)을\n",
        "    추출합니다. 복합 TLD(예: co.uk)의 경우 마지막 부분(uk)만 추출합니다.\n",
        "\n",
        "    필수 전제조건: response() 메서드가 먼저 호출되어 self._response_list가 채워져 있어야 합니다.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: 추출된 모든 TLD의 리스트\n",
        "\n",
        "    Raises:\n",
        "        ValueError: response_list가 설정되지 않은 경우\n",
        "    \"\"\"\n",
        "    if (self._response_list is None):\n",
        "      raise ValueError(\"response_list must be set\")\n",
        "\n",
        "    self._tlds = []\n",
        "    for url in self._response_list:\n",
        "        tld = tldextract.extract(url).suffix\n",
        "        if (len(tld.split('.')) > 1):\n",
        "            self._tlds.append(tld.split('.')[-1])\n",
        "        else:\n",
        "            self._tlds.append(tld)\n",
        "    return self._tlds\n",
        "\n",
        "  def extract_stlds(self):\n",
        "    \"\"\"\n",
        "    수집된 URL 목록에서 2단계 최상위 도메인(STLD) 전체를 추출합니다.\n",
        "\n",
        "    각 URL에서 tldextract 라이브러리를 사용하여 전체 최상위 도메인(예: com, co.uk)을\n",
        "    추출합니다. TLD와 달리 복합 도메인(co.uk, com.au 등)을 그대로 유지합니다.\n",
        "\n",
        "    필수 전제조건: response() 메서드가 먼저 호출되어 self._response_list가 채워져 있어야 합니다.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: 추출된 모든 STLD의 리스트\n",
        "\n",
        "    Raises:\n",
        "        ValueError: response_list가 설정되지 않은 경우\n",
        "    \"\"\"\n",
        "    if (self._response_list is None):\n",
        "      raise ValueError(\"response_list must be set\")\n",
        "\n",
        "    self._stlds = [tldextract.extract(url).suffix for url in self._response_list]\n",
        "    return self._stlds\n",
        "\n",
        "  def extract_domains(self):\n",
        "    \"\"\"\n",
        "    수집된 URL 목록에서 전체 도메인 이름을 추출합니다.\n",
        "\n",
        "    각 URL에서 tldextract 라이브러리를 사용하여 도메인과 TLD를 결합한\n",
        "    전체 도메인 이름(예: google.com, bbc.co.uk)을 추출합니다.\n",
        "    서브도메인은 제외됩니다.\n",
        "\n",
        "    필수 전제조건: response() 메서드가 먼저 호출되어 self._response_list가 채워져 있어야 합니다.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: 추출된 모든 도메인 이름의 리스트\n",
        "    \"\"\"\n",
        "    self._domains = []\n",
        "    for url in self._response_list:\n",
        "        d = f\"{tldextract.extract(url).domain}.{tldextract.extract(url).suffix}\"\n",
        "        self._domains.append(d)\n",
        "    return self._domains\n",
        "\n",
        "  def count_tlds(self):\n",
        "    \"\"\"\n",
        "    추출된 최상위 도메인(TLD)의 출현 빈도를 계산하고 시각화합니다.\n",
        "\n",
        "    TLD 목록에서 각 TLD의 등장 횟수를 계산하여 pandas DataFrame으로 변환하고,\n",
        "    빈도수가 높은 상위 20개 TLD를 막대 그래프로 시각화합니다.\n",
        "    각 막대 위에 정확한 빈도수를 텍스트로 표시합니다.\n",
        "\n",
        "    필수 전제조건: extract_tld() 메서드가 먼저 호출되어 self._tlds가 채워져 있어야 합니다.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: TLD와 해당 빈도수를 포함하는 DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    self._count_tlds = pd.DataFrame({\"tld\" : dict(Counter(self._tlds)).keys(),\n",
        "                                   \"value\": dict(Counter(self._tlds)).values()})\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    tld_df = self._count_tlds.sort_values(by='value', ascending=False).head(20)\n",
        "    ax = sns.barplot(x='tld', y='value', data=tld_df, palette='viridis')\n",
        "    plt.title('Top 20 Top-Level Domains (TLDs)')\n",
        "    plt.xlabel('TLD')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    for i, v in enumerate(tld_df['value']):\n",
        "        ax.text(i, v+0.1, str(v), ha='center')\n",
        "    plt.show()\n",
        "\n",
        "    return self._count_tlds\n",
        "\n",
        "  def count_stlds(self):\n",
        "    \"\"\"\n",
        "    추출된 2단계 최상위 도메인(STLD)의 출현 빈도를 계산하고 시각화합니다.\n",
        "\n",
        "    STLD 목록에서 각 STLD의 등장 횟수를 계산하여 pandas DataFrame으로 변환하고,\n",
        "    빈도수가 높은 상위 20개 STLD를 막대 그래프로 시각화합니다.\n",
        "    각 막대 위에 정확한 빈도수를 텍스트로 표시합니다.\n",
        "\n",
        "    필수 전제조건: extract_stlds() 메서드가 먼저 호출되어 self._stlds가 채워져 있어야 합니다.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: STLD와 해당 빈도수를 포함하는 DataFrame\n",
        "    \"\"\"\n",
        "    self._count_stlds = pd.DataFrame({\"stld\": dict(Counter(self._stlds)).keys(),\n",
        "                                   \"value\": dict(Counter(self._stlds)).values()})\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    stld_df = self._count_stlds.sort_values(by='value', ascending=False).head(20)\n",
        "    ax = sns.barplot(x='stld', y='value', data=stld_df, palette='coolwarm')\n",
        "    plt.title('Top 20 Second-Level Domains (STLDs)')\n",
        "    plt.xlabel('STLD')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    for i, v in enumerate(stld_df['value']):\n",
        "        ax.text(i, v+0.1, str(v), ha='center')\n",
        "    plt.show()\n",
        "\n",
        "    return self._count_stlds\n",
        "\n",
        "  def count_domains(self):\n",
        "    \"\"\"\n",
        "    추출된 도메인의 출현 빈도를 계산하고 시각화합니다.\n",
        "\n",
        "    도메인 목록에서 각 도메인의 등장 횟수를 계산하여 pandas DataFrame으로 변환하고,\n",
        "    빈도수가 높은 상위 20개 도메인을 막대 그래프로 시각화합니다.\n",
        "    각 막대 위에 정확한 빈도수를 텍스트로 표시합니다.\n",
        "\n",
        "    필수 전제조건: extract_domains() 메서드가 먼저 호출되어 self._domains가 채워져 있어야 합니다.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: 도메인과 해당 빈도수를 포함하는 DataFrame\n",
        "    \"\"\"\n",
        "    self._count_domains = pd.DataFrame({\"domains\": dict(Counter(self._domains)).keys(),\n",
        "                                   \"value\": dict(Counter(self._domains)).values()})\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    domain_df = self._count_domains.sort_values(by='value', ascending=False).head(20)\n",
        "    ax = sns.barplot(x='domains', y='value', data=domain_df, palette='coolwarm')\n",
        "    plt.title('Top 20 Domains')\n",
        "    plt.xlabel('Domain')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    for i, v in enumerate(domain_df['value']):\n",
        "        ax.text(i, v+0.1, str(v), ha='center')\n",
        "    plt.show()\n",
        "\n",
        "    return self._count_domains\n",
        "\n",
        "  def save_to_csv(self):\n",
        "    \"\"\"\n",
        "    수집 및 분석된 모든 데이터를 CSV 파일로 저장합니다.\n",
        "\n",
        "    다음 네 가지 파일을 생성합니다:\n",
        "    1. response_list.csv: 수집된 모든 URL 목록\n",
        "    2. count_tlds.csv: TLD 빈도 분석 결과\n",
        "    3. count_stlds.csv: STLD 빈도 분석 결과\n",
        "    4. count_domains.csv: 도메인 빈도 분석 결과\n",
        "\n",
        "    각 저장 작업은 독립적으로 수행되며, 한 파일 저장에 실패해도\n",
        "    다른 파일 저장 작업은 계속 진행됩니다.\n",
        "\n",
        "    필수 전제조건: 해당 메서드 호출 전에 관련 데이터가 생성되어 있어야 합니다.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    try:\n",
        "      print(\"Saving raw link list ...\")\n",
        "      with open('response_list.csv', 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(self._response_list)\n",
        "    except Exception as e:\n",
        "      print(f\"Error: {e}\")\n",
        "\n",
        "    try:\n",
        "      print(\"Saving tlds count ...\")\n",
        "      self._count_tlds.to_csv('count_tlds.csv', index=False)\n",
        "    except Exception as e:\n",
        "      print(f\"Error: {e}\")\n",
        "\n",
        "    try:\n",
        "      print(\"Saving stlds count ...\")\n",
        "      self._count_stlds.to_csv('count_stlds.csv', index=False)\n",
        "    except Exception as e:\n",
        "      print(f\"Error: {e}\")\n",
        "\n",
        "    try:\n",
        "      print(\"Saving domains count ...\")\n",
        "      self._count_domains.to_csv('count_domains.csv', index=False)\n",
        "    except Exception as e:\n",
        "      print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "Gf-lAc-cUmaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실습"
      ],
      "metadata": {
        "id": "S00l1adiCC7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cse = googlecse()\n",
        "#cse.setup_search_parameters()\n",
        "\n",
        "cse.API_KEY = \"PUT_API_KEY_HERE\"\n",
        "cse.SEARCH_ENGINE_ID = \"PUT_SEARCH_ENGINE_ID_HERE\"\n",
        "cse.query = \"PUT_QUERY_HERE\"\n",
        "print(cse)"
      ],
      "metadata": {
        "id": "TSQJsbZQXUZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responese = cse.response_oneline()"
      ],
      "metadata": {
        "id": "JV7gAi_bbNai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = cse.response()\n",
        "print(\"\\u2501\" * 80)\n",
        "response"
      ],
      "metadata": {
        "id": "6BtILECEcwDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cse.extract_tld())\n",
        "print(cse.extract_stlds())\n",
        "print(cse.extract_domains())"
      ],
      "metadata": {
        "id": "OycF5JimlI4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cse.count_tlds()"
      ],
      "metadata": {
        "id": "u7k0rBV5ljU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cse.count_stlds()"
      ],
      "metadata": {
        "id": "mCZRsZvguYuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cse.count_domains()"
      ],
      "metadata": {
        "id": "J3SoalfDubhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cse.save_to_csv()"
      ],
      "metadata": {
        "id": "4X5mFWenudoh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}